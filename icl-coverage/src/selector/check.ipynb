{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import datasets\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model\n",
    ")\n",
    "from datasets import Dataset\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pickle\n",
      "{'runtime': defaultdict(<class 'list'>, {'identity': 2.86102294921875e-06, 'proposed': 76.37844920158386}), 'influence': defaultdict(<class 'list'>, {'identity':           0          1          2          3          4          5   \\\n",
      "0  32.820591  24.366282  20.888435  24.008913  27.876163  19.330626   \n",
      "1  24.366282  34.807888  16.800751  19.194273  29.856312  14.258027   \n",
      "2  20.888435  16.800751  27.595709  30.788624  16.551535  24.993111   \n",
      "3  24.008913  19.194273  30.788624  43.560310  18.507236  31.541586   \n",
      "4  27.876163  29.856312  16.551535  18.507236  44.374142  13.761898   \n",
      "5  19.330626  14.258027  24.993111  31.541586  13.761898  30.109024   \n",
      "6  17.338789  16.158508  16.353416  18.829626  17.886845  15.359276   \n",
      "7   3.519126   2.729677   4.867253   6.103373   2.321034   5.158979   \n",
      "8   2.335143   1.782158   3.156395   3.924849   1.622480   3.205610   \n",
      "9  16.690983  13.632009  21.556187  26.749725  12.114012  21.738220   \n",
      "\n",
      "          6         7         8          9   ...         80         81  \\\n",
      "0  17.338789  3.519126  2.335143  16.690983  ...  20.262615   6.594930   \n",
      "1  16.158508  2.729677  1.782158  13.632009  ...  14.700734   4.840516   \n",
      "2  16.353416  4.867253  3.156395  21.556187  ...  26.560804   8.110387   \n",
      "3  18.829626  6.103373  3.924849  26.749725  ...  34.213810  10.014878   \n",
      "4  17.886845  2.321034  1.622480  12.114012  ...  13.807520   4.628916   \n",
      "5  15.359276  5.158979  3.205610  21.738220  ...  27.565596   8.591434   \n",
      "6  15.916228  2.820442  1.855622  13.582916  ...  16.046078   4.957475   \n",
      "7   2.820442  1.998002  0.706836   4.442897  ...   5.389064   1.848344   \n",
      "8   1.855622  0.706836  0.816213   2.798891  ...   3.480939   1.119218   \n",
      "9  13.582916  4.442897  2.798891  22.185104  ...  23.281120   7.151941   \n",
      "\n",
      "          82         83         84         85         86         87  \\\n",
      "0  28.385565  20.162497  18.866760  21.192972  24.346670  23.240385   \n",
      "1  33.091309  15.596599  13.757977  16.760941  22.986389  19.872904   \n",
      "2  18.569006  25.056971  25.223063  24.640476  25.623606  26.064585   \n",
      "3  20.386045  31.470318  31.818426  30.918308  30.114838  31.570856   \n",
      "4  36.079967  15.075655  13.276291  17.503263  23.988661  20.674328   \n",
      "5  15.047338  25.955151  26.168173  25.429789  24.568571  25.841841   \n",
      "6  18.973166  15.532969  15.325832  16.096668  19.027748  18.092476   \n",
      "7   2.742231   5.179906   5.113521   4.884847   4.664323   5.102140   \n",
      "8   1.868726   3.214718   3.287251   3.125793   3.150348   3.213820   \n",
      "9  14.735616  21.702389  22.590458  21.032110  21.691521  22.835573   \n",
      "\n",
      "          88         89  \n",
      "0  28.257452  20.262136  \n",
      "1  29.782211  13.887803  \n",
      "2  18.517769  30.585463  \n",
      "3  21.580839  39.774551  \n",
      "4  32.224976  10.879612  \n",
      "5  16.084562  32.581600  \n",
      "6  17.163710  16.660625  \n",
      "7   3.001694   6.540792  \n",
      "8   2.038152   4.065738  \n",
      "9  14.395902  28.018728  \n",
      "\n",
      "[10 rows x 90 columns], 'proposed':              0             1             2           3             4   \\\n",
      "0  5.369296e+07  1.864223e+07  1.587794e+07  15744662.0  2.337107e+07   \n",
      "1  1.864223e+07  6.306413e+07  1.467192e+07  14364215.0  2.149901e+07   \n",
      "2  1.587794e+07  1.467192e+07  3.041363e+07  15628406.0  1.213848e+07   \n",
      "3  1.574466e+07  1.436422e+07  1.562841e+07  37648312.0  1.536992e+07   \n",
      "4  2.337107e+07  2.149902e+07  1.213848e+07  15369917.0  7.453458e+07   \n",
      "5  1.307388e+07  1.084192e+07  1.262898e+07  14911208.0  1.027943e+07   \n",
      "6  1.246244e+07  1.305836e+07  1.117914e+07   9699730.0  1.256010e+07   \n",
      "7  2.047420e+06  1.914238e+06  1.976425e+06   2320512.0  1.613368e+06   \n",
      "8  1.480638e+06  1.166030e+06  1.841605e+06   1568856.5  1.077624e+06   \n",
      "9  1.103410e+07  1.595546e+07  1.132429e+07  11200064.0  1.042078e+07   \n",
      "\n",
      "             5            6             7             8             9   ...  \\\n",
      "0  1.307388e+07  12462436.00  2.047421e+06  1.480638e+06  1.103410e+07  ...   \n",
      "1  1.084192e+07  13058361.00  1.914238e+06  1.166030e+06  1.595546e+07  ...   \n",
      "2  1.262898e+07  11179142.00  1.976425e+06  1.841605e+06  1.132429e+07  ...   \n",
      "3  1.491121e+07   9699731.00  2.320512e+06  1.568856e+06  1.120006e+07  ...   \n",
      "4  1.027943e+07  12560100.00  1.613368e+06  1.077624e+06  1.042078e+07  ...   \n",
      "5  3.475765e+07   8967364.00  2.082193e+06  1.439486e+06  8.441258e+06  ...   \n",
      "6  8.967364e+06  24378538.00  1.582759e+06  9.096949e+05  8.708556e+06  ...   \n",
      "7  2.082192e+06   1582758.75  4.426112e+06  4.799805e+05  1.790332e+06  ...   \n",
      "8  1.439486e+06    909695.00  4.799805e+05  2.247624e+06  1.253992e+06  ...   \n",
      "9  8.441258e+06   8708556.00  1.790332e+06  1.253992e+06  2.381760e+07  ...   \n",
      "\n",
      "             80          81            82           83            84  \\\n",
      "0  1.449626e+07  4313669.00  2.171263e+07  12424886.00  1.190995e+07   \n",
      "1  9.756334e+06  2840394.75  3.312994e+07  10167306.00  1.080846e+07   \n",
      "2  1.201361e+07  3662364.75  1.697507e+07  12447895.00  1.136917e+07   \n",
      "3  1.488303e+07  4534958.50  1.383546e+07  14526054.00  1.402156e+07   \n",
      "4  8.925205e+06  2229767.75  2.496263e+07   9058691.00  1.241946e+07   \n",
      "5  1.311982e+07  4242632.00  1.113523e+07  13511962.00  1.203101e+07   \n",
      "6  8.515708e+06  3217584.50  1.478903e+07   8435636.00  8.065449e+06   \n",
      "7  2.300134e+06  1365988.00  2.025848e+06   2933412.00  1.445238e+06   \n",
      "8  1.718385e+06   496590.00  1.751867e+06   1490252.25  1.576360e+06   \n",
      "9  8.716714e+06  2705527.00  1.932787e+07   9268257.00  8.754935e+06   \n",
      "\n",
      "             85            86          87            88            89  \n",
      "0  1.314032e+07  1.166917e+07  15867419.0  3.031401e+07  1.467453e+07  \n",
      "1  9.237422e+06  1.696427e+07  16871956.0  2.531188e+07  1.273090e+07  \n",
      "2  1.371259e+07  1.475178e+07  16791552.0  1.608855e+07  1.342173e+07  \n",
      "3  1.536848e+07  1.324043e+07  15805592.0  1.784075e+07  1.845760e+07  \n",
      "4  1.091620e+07  1.781816e+07  17013272.0  2.383130e+07  1.143481e+07  \n",
      "5  1.410162e+07  1.305965e+07  13367531.0  1.320005e+07  1.419166e+07  \n",
      "6  7.978068e+06  1.327048e+07  12654260.0  1.323501e+07  8.780131e+06  \n",
      "7  2.288231e+06  1.292179e+06   2788804.5  1.508933e+06  2.061081e+06  \n",
      "8  1.682907e+06  1.453890e+06   1933381.5  1.581276e+06  1.547585e+06  \n",
      "9  8.645032e+06  8.760630e+06  14684292.0  1.280562e+07  1.177658e+07  \n",
      "\n",
      "[10 rows x 90 columns]})}\n",
      "   0   1   2   3   4   5   6   7   8   9   ...  80  81  82  83  84  85  86  \\\n",
      "0  32   8   7  11  61  81  45  72  25  63  ...  82  46  12   4  48  70  69   \n",
      "1  32   8   7  61  81  25  11  59  27  24  ...  55  12  36  46  70  40  88   \n",
      "2  32   8   7  11  81  61  59  45  43  25  ...  76  88  35  65  70  87  82   \n",
      "3   8  32   7  11  61  81  45  59  75  25  ...   0  87  69  77  88  89  35   \n",
      "4  32   8   7  61  81  11  59  25  63  45  ...  40  88  55  70  82  12  46   \n",
      "5   8  32   7  11  61  81  45  59  24  63  ...  85  70  14  89  68   3  12   \n",
      "6  32   8   7  11  61  81  10  27  45  25  ...  14  30  75  82  46  48  12   \n",
      "7   8  32  11  14  55  61  25  24  86  81  ...  42  57  87  64  30  74  83   \n",
      "8  32   7  81  61  11  59  47  28  72  25  ...  71  66  82  18   2  77  54   \n",
      "9  32   8   7  61  81  11  25  45  59  27  ...  46  77  65  35  88  87   1   \n",
      "\n",
      "   87  88  89  \n",
      "0  14  88   0  \n",
      "1  14  82   1  \n",
      "2  36  34   2  \n",
      "3  34  14   3  \n",
      "4  79  14   4  \n",
      "5  69  76   5  \n",
      "6  55  70   6  \n",
      "7  18  49   7  \n",
      "8  23  87   8  \n",
      "9  64  82   9  \n",
      "\n",
      "[10 rows x 90 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('results_save_influence0.pkl', 'rb') as f:\n",
    "    IF_dict=pkl.load(f)\n",
    "print('loaded pickle')\n",
    "print(IF_dict)\n",
    "sorted_influences=IF_dict['influence']['proposed'].apply(lambda x: x.argsort(), axis=1)\n",
    "print(sorted_influences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.load_from_disk(\"/nas02/Hadi/Incontenxt-influence/DataInf/datasets/banking77-train-900.hf\")\n",
    "validation_dataset = datasets.load_from_disk(\"/nas02/Hadi/Incontenxt-influence/DataInf/datasets/banking77-test-100.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes=set(train_dataset['label_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Refund_not_showing_up',\n",
       " 'activate_my_card',\n",
       " 'age_limit',\n",
       " 'apple_pay_or_google_pay',\n",
       " 'atm_support',\n",
       " 'automatic_top_up',\n",
       " 'balance_not_updated_after_bank_transfer',\n",
       " 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 'beneficiary_not_allowed',\n",
       " 'cancel_transfer',\n",
       " 'card_about_to_expire',\n",
       " 'card_acceptance',\n",
       " 'card_arrival',\n",
       " 'card_delivery_estimate',\n",
       " 'card_linking',\n",
       " 'card_not_working',\n",
       " 'card_payment_fee_charged',\n",
       " 'card_payment_not_recognised',\n",
       " 'card_payment_wrong_exchange_rate',\n",
       " 'card_swallowed',\n",
       " 'cash_withdrawal_charge',\n",
       " 'cash_withdrawal_not_recognised',\n",
       " 'change_pin',\n",
       " 'compromised_card',\n",
       " 'contactless_not_working',\n",
       " 'country_support',\n",
       " 'declined_card_payment',\n",
       " 'declined_cash_withdrawal',\n",
       " 'declined_transfer',\n",
       " 'direct_debit_payment_not_recognised',\n",
       " 'disposable_card_limits',\n",
       " 'edit_personal_details',\n",
       " 'exchange_charge',\n",
       " 'exchange_rate',\n",
       " 'exchange_via_app',\n",
       " 'extra_charge_on_statement',\n",
       " 'failed_transfer',\n",
       " 'fiat_currency_support',\n",
       " 'get_disposable_virtual_card',\n",
       " 'get_physical_card',\n",
       " 'getting_spare_card',\n",
       " 'getting_virtual_card',\n",
       " 'lost_or_stolen_card',\n",
       " 'lost_or_stolen_phone',\n",
       " 'order_physical_card',\n",
       " 'passcode_forgotten',\n",
       " 'pending_card_payment',\n",
       " 'pending_cash_withdrawal',\n",
       " 'pending_top_up',\n",
       " 'pending_transfer',\n",
       " 'pin_blocked',\n",
       " 'receiving_money',\n",
       " 'request_refund',\n",
       " 'reverted_card_payment?',\n",
       " 'supported_cards_and_currencies',\n",
       " 'terminate_account',\n",
       " 'top_up_by_bank_transfer_charge',\n",
       " 'top_up_by_card_charge',\n",
       " 'top_up_by_cash_or_cheque',\n",
       " 'top_up_failed',\n",
       " 'top_up_limits',\n",
       " 'top_up_reverted',\n",
       " 'topping_up_by_card',\n",
       " 'transaction_charged_twice',\n",
       " 'transfer_fee_charged',\n",
       " 'transfer_into_account',\n",
       " 'transfer_not_received_by_recipient',\n",
       " 'transfer_timing',\n",
       " 'unable_to_verify_identity',\n",
       " 'verify_my_identity',\n",
       " 'verify_source_of_funds',\n",
       " 'verify_top_up',\n",
       " 'virtual_card_not_working',\n",
       " 'visa_or_mastercard',\n",
       " 'why_verify_identity',\n",
       " 'wrong_amount_of_cash_received',\n",
       " 'wrong_exchange_rate_for_cash_withdrawal'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id_mapping={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in enumerate(train_dataset['label_text']):\n",
    "    class_id_mapping[k]=v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'apple_pay_or_google_pay',\n",
       " 1: 'apple_pay_or_google_pay',\n",
       " 2: 'apple_pay_or_google_pay',\n",
       " 3: 'apple_pay_or_google_pay',\n",
       " 4: 'apple_pay_or_google_pay',\n",
       " 5: 'apple_pay_or_google_pay',\n",
       " 6: 'apple_pay_or_google_pay',\n",
       " 7: 'apple_pay_or_google_pay',\n",
       " 8: 'apple_pay_or_google_pay',\n",
       " 9: 'apple_pay_or_google_pay',\n",
       " 10: 'apple_pay_or_google_pay',\n",
       " 11: 'apple_pay_or_google_pay',\n",
       " 12: 'supported_cards_and_currencies',\n",
       " 13: 'supported_cards_and_currencies',\n",
       " 14: 'supported_cards_and_currencies',\n",
       " 15: 'supported_cards_and_currencies',\n",
       " 16: 'supported_cards_and_currencies',\n",
       " 17: 'supported_cards_and_currencies',\n",
       " 18: 'supported_cards_and_currencies',\n",
       " 19: 'supported_cards_and_currencies',\n",
       " 20: 'supported_cards_and_currencies',\n",
       " 21: 'supported_cards_and_currencies',\n",
       " 22: 'supported_cards_and_currencies',\n",
       " 23: 'supported_cards_and_currencies',\n",
       " 24: 'pending_cash_withdrawal',\n",
       " 25: 'pending_cash_withdrawal',\n",
       " 26: 'pending_cash_withdrawal',\n",
       " 27: 'pending_cash_withdrawal',\n",
       " 28: 'pending_cash_withdrawal',\n",
       " 29: 'pending_cash_withdrawal',\n",
       " 30: 'pending_cash_withdrawal',\n",
       " 31: 'pending_cash_withdrawal',\n",
       " 32: 'pending_cash_withdrawal',\n",
       " 33: 'pending_cash_withdrawal',\n",
       " 34: 'pending_cash_withdrawal',\n",
       " 35: 'pending_cash_withdrawal',\n",
       " 36: 'terminate_account',\n",
       " 37: 'terminate_account',\n",
       " 38: 'terminate_account',\n",
       " 39: 'terminate_account',\n",
       " 40: 'terminate_account',\n",
       " 41: 'terminate_account',\n",
       " 42: 'terminate_account',\n",
       " 43: 'terminate_account',\n",
       " 44: 'terminate_account',\n",
       " 45: 'terminate_account',\n",
       " 46: 'terminate_account',\n",
       " 47: 'terminate_account',\n",
       " 48: 'receiving_money',\n",
       " 49: 'receiving_money',\n",
       " 50: 'receiving_money',\n",
       " 51: 'receiving_money',\n",
       " 52: 'receiving_money',\n",
       " 53: 'receiving_money',\n",
       " 54: 'receiving_money',\n",
       " 55: 'receiving_money',\n",
       " 56: 'receiving_money',\n",
       " 57: 'receiving_money',\n",
       " 58: 'receiving_money',\n",
       " 59: 'receiving_money',\n",
       " 60: 'top_up_by_bank_transfer_charge',\n",
       " 61: 'top_up_by_bank_transfer_charge',\n",
       " 62: 'top_up_by_bank_transfer_charge',\n",
       " 63: 'top_up_by_bank_transfer_charge',\n",
       " 64: 'top_up_by_bank_transfer_charge',\n",
       " 65: 'top_up_by_bank_transfer_charge',\n",
       " 66: 'top_up_by_bank_transfer_charge',\n",
       " 67: 'top_up_by_bank_transfer_charge',\n",
       " 68: 'top_up_by_bank_transfer_charge',\n",
       " 69: 'top_up_by_bank_transfer_charge',\n",
       " 70: 'top_up_by_bank_transfer_charge',\n",
       " 71: 'top_up_by_bank_transfer_charge',\n",
       " 72: 'contactless_not_working',\n",
       " 73: 'contactless_not_working',\n",
       " 74: 'contactless_not_working',\n",
       " 75: 'contactless_not_working',\n",
       " 76: 'contactless_not_working',\n",
       " 77: 'contactless_not_working',\n",
       " 78: 'contactless_not_working',\n",
       " 79: 'contactless_not_working',\n",
       " 80: 'contactless_not_working',\n",
       " 81: 'contactless_not_working',\n",
       " 82: 'contactless_not_working',\n",
       " 83: 'contactless_not_working',\n",
       " 84: 'card_arrival',\n",
       " 85: 'card_arrival',\n",
       " 86: 'card_arrival',\n",
       " 87: 'card_arrival',\n",
       " 88: 'card_arrival',\n",
       " 89: 'card_arrival',\n",
       " 90: 'card_arrival',\n",
       " 91: 'card_arrival',\n",
       " 92: 'card_arrival',\n",
       " 93: 'card_arrival',\n",
       " 94: 'card_arrival',\n",
       " 95: 'card_arrival',\n",
       " 96: 'fiat_currency_support',\n",
       " 97: 'fiat_currency_support',\n",
       " 98: 'fiat_currency_support',\n",
       " 99: 'fiat_currency_support',\n",
       " 100: 'fiat_currency_support',\n",
       " 101: 'fiat_currency_support',\n",
       " 102: 'fiat_currency_support',\n",
       " 103: 'fiat_currency_support',\n",
       " 104: 'fiat_currency_support',\n",
       " 105: 'fiat_currency_support',\n",
       " 106: 'fiat_currency_support',\n",
       " 107: 'fiat_currency_support',\n",
       " 108: 'pending_card_payment',\n",
       " 109: 'pending_card_payment',\n",
       " 110: 'pending_card_payment',\n",
       " 111: 'pending_card_payment',\n",
       " 112: 'pending_card_payment',\n",
       " 113: 'pending_card_payment',\n",
       " 114: 'pending_card_payment',\n",
       " 115: 'pending_card_payment',\n",
       " 116: 'pending_card_payment',\n",
       " 117: 'pending_card_payment',\n",
       " 118: 'pending_card_payment',\n",
       " 119: 'pending_card_payment',\n",
       " 120: 'beneficiary_not_allowed',\n",
       " 121: 'beneficiary_not_allowed',\n",
       " 122: 'beneficiary_not_allowed',\n",
       " 123: 'beneficiary_not_allowed',\n",
       " 124: 'beneficiary_not_allowed',\n",
       " 125: 'beneficiary_not_allowed',\n",
       " 126: 'beneficiary_not_allowed',\n",
       " 127: 'beneficiary_not_allowed',\n",
       " 128: 'beneficiary_not_allowed',\n",
       " 129: 'beneficiary_not_allowed',\n",
       " 130: 'beneficiary_not_allowed',\n",
       " 131: 'beneficiary_not_allowed',\n",
       " 132: 'transfer_fee_charged',\n",
       " 133: 'transfer_fee_charged',\n",
       " 134: 'transfer_fee_charged',\n",
       " 135: 'transfer_fee_charged',\n",
       " 136: 'transfer_fee_charged',\n",
       " 137: 'transfer_fee_charged',\n",
       " 138: 'transfer_fee_charged',\n",
       " 139: 'transfer_fee_charged',\n",
       " 140: 'transfer_fee_charged',\n",
       " 141: 'transfer_fee_charged',\n",
       " 142: 'transfer_fee_charged',\n",
       " 143: 'transfer_fee_charged',\n",
       " 144: 'transfer_not_received_by_recipient',\n",
       " 145: 'transfer_not_received_by_recipient',\n",
       " 146: 'transfer_not_received_by_recipient',\n",
       " 147: 'transfer_not_received_by_recipient',\n",
       " 148: 'transfer_not_received_by_recipient',\n",
       " 149: 'transfer_not_received_by_recipient',\n",
       " 150: 'transfer_not_received_by_recipient',\n",
       " 151: 'transfer_not_received_by_recipient',\n",
       " 152: 'transfer_not_received_by_recipient',\n",
       " 153: 'transfer_not_received_by_recipient',\n",
       " 154: 'transfer_not_received_by_recipient',\n",
       " 155: 'transfer_not_received_by_recipient',\n",
       " 156: 'atm_support',\n",
       " 157: 'atm_support',\n",
       " 158: 'atm_support',\n",
       " 159: 'atm_support',\n",
       " 160: 'atm_support',\n",
       " 161: 'atm_support',\n",
       " 162: 'atm_support',\n",
       " 163: 'atm_support',\n",
       " 164: 'atm_support',\n",
       " 165: 'atm_support',\n",
       " 166: 'atm_support',\n",
       " 167: 'atm_support',\n",
       " 168: 'getting_spare_card',\n",
       " 169: 'getting_spare_card',\n",
       " 170: 'getting_spare_card',\n",
       " 171: 'getting_spare_card',\n",
       " 172: 'getting_spare_card',\n",
       " 173: 'getting_spare_card',\n",
       " 174: 'getting_spare_card',\n",
       " 175: 'getting_spare_card',\n",
       " 176: 'getting_spare_card',\n",
       " 177: 'getting_spare_card',\n",
       " 178: 'getting_spare_card',\n",
       " 179: 'getting_spare_card',\n",
       " 180: 'country_support',\n",
       " 181: 'country_support',\n",
       " 182: 'country_support',\n",
       " 183: 'country_support',\n",
       " 184: 'country_support',\n",
       " 185: 'country_support',\n",
       " 186: 'country_support',\n",
       " 187: 'country_support',\n",
       " 188: 'country_support',\n",
       " 189: 'country_support',\n",
       " 190: 'country_support',\n",
       " 191: 'country_support',\n",
       " 192: 'verify_source_of_funds',\n",
       " 193: 'verify_source_of_funds',\n",
       " 194: 'verify_source_of_funds',\n",
       " 195: 'verify_source_of_funds',\n",
       " 196: 'verify_source_of_funds',\n",
       " 197: 'verify_source_of_funds',\n",
       " 198: 'verify_source_of_funds',\n",
       " 199: 'verify_source_of_funds',\n",
       " 200: 'verify_source_of_funds',\n",
       " 201: 'verify_source_of_funds',\n",
       " 202: 'verify_source_of_funds',\n",
       " 203: 'verify_source_of_funds',\n",
       " 204: 'pending_transfer',\n",
       " 205: 'pending_transfer',\n",
       " 206: 'pending_transfer',\n",
       " 207: 'pending_transfer',\n",
       " 208: 'pending_transfer',\n",
       " 209: 'pending_transfer',\n",
       " 210: 'pending_transfer',\n",
       " 211: 'pending_transfer',\n",
       " 212: 'pending_transfer',\n",
       " 213: 'pending_transfer',\n",
       " 214: 'pending_transfer',\n",
       " 215: 'pending_transfer',\n",
       " 216: 'edit_personal_details',\n",
       " 217: 'edit_personal_details',\n",
       " 218: 'edit_personal_details',\n",
       " 219: 'edit_personal_details',\n",
       " 220: 'edit_personal_details',\n",
       " 221: 'edit_personal_details',\n",
       " 222: 'edit_personal_details',\n",
       " 223: 'edit_personal_details',\n",
       " 224: 'edit_personal_details',\n",
       " 225: 'edit_personal_details',\n",
       " 226: 'edit_personal_details',\n",
       " 227: 'edit_personal_details',\n",
       " 228: 'get_disposable_virtual_card',\n",
       " 229: 'get_disposable_virtual_card',\n",
       " 230: 'get_disposable_virtual_card',\n",
       " 231: 'get_disposable_virtual_card',\n",
       " 232: 'get_disposable_virtual_card',\n",
       " 233: 'get_disposable_virtual_card',\n",
       " 234: 'get_disposable_virtual_card',\n",
       " 235: 'get_disposable_virtual_card',\n",
       " 236: 'get_disposable_virtual_card',\n",
       " 237: 'get_disposable_virtual_card',\n",
       " 238: 'get_disposable_virtual_card',\n",
       " 239: 'get_disposable_virtual_card',\n",
       " 240: 'card_acceptance',\n",
       " 241: 'card_acceptance',\n",
       " 242: 'card_acceptance',\n",
       " 243: 'card_acceptance',\n",
       " 244: 'card_acceptance',\n",
       " 245: 'card_acceptance',\n",
       " 246: 'card_acceptance',\n",
       " 247: 'card_acceptance',\n",
       " 248: 'card_acceptance',\n",
       " 249: 'card_acceptance',\n",
       " 250: 'card_acceptance',\n",
       " 251: 'card_acceptance',\n",
       " 252: 'card_about_to_expire',\n",
       " 253: 'card_about_to_expire',\n",
       " 254: 'card_about_to_expire',\n",
       " 255: 'card_about_to_expire',\n",
       " 256: 'card_about_to_expire',\n",
       " 257: 'card_about_to_expire',\n",
       " 258: 'card_about_to_expire',\n",
       " 259: 'card_about_to_expire',\n",
       " 260: 'card_about_to_expire',\n",
       " 261: 'card_about_to_expire',\n",
       " 262: 'card_about_to_expire',\n",
       " 263: 'card_about_to_expire',\n",
       " 264: 'compromised_card',\n",
       " 265: 'compromised_card',\n",
       " 266: 'compromised_card',\n",
       " 267: 'compromised_card',\n",
       " 268: 'compromised_card',\n",
       " 269: 'compromised_card',\n",
       " 270: 'compromised_card',\n",
       " 271: 'compromised_card',\n",
       " 272: 'compromised_card',\n",
       " 273: 'compromised_card',\n",
       " 274: 'compromised_card',\n",
       " 275: 'compromised_card',\n",
       " 276: 'reverted_card_payment?',\n",
       " 277: 'reverted_card_payment?',\n",
       " 278: 'reverted_card_payment?',\n",
       " 279: 'reverted_card_payment?',\n",
       " 280: 'reverted_card_payment?',\n",
       " 281: 'reverted_card_payment?',\n",
       " 282: 'reverted_card_payment?',\n",
       " 283: 'reverted_card_payment?',\n",
       " 284: 'reverted_card_payment?',\n",
       " 285: 'reverted_card_payment?',\n",
       " 286: 'reverted_card_payment?',\n",
       " 287: 'reverted_card_payment?',\n",
       " 288: 'why_verify_identity',\n",
       " 289: 'why_verify_identity',\n",
       " 290: 'why_verify_identity',\n",
       " 291: 'why_verify_identity',\n",
       " 292: 'why_verify_identity',\n",
       " 293: 'why_verify_identity',\n",
       " 294: 'why_verify_identity',\n",
       " 295: 'why_verify_identity',\n",
       " 296: 'why_verify_identity',\n",
       " 297: 'why_verify_identity',\n",
       " 298: 'why_verify_identity',\n",
       " 299: 'why_verify_identity',\n",
       " 300: 'pending_top_up',\n",
       " 301: 'pending_top_up',\n",
       " 302: 'pending_top_up',\n",
       " 303: 'pending_top_up',\n",
       " 304: 'pending_top_up',\n",
       " 305: 'pending_top_up',\n",
       " 306: 'pending_top_up',\n",
       " 307: 'pending_top_up',\n",
       " 308: 'pending_top_up',\n",
       " 309: 'pending_top_up',\n",
       " 310: 'pending_top_up',\n",
       " 311: 'pending_top_up',\n",
       " 312: 'cash_withdrawal_not_recognised',\n",
       " 313: 'cash_withdrawal_not_recognised',\n",
       " 314: 'cash_withdrawal_not_recognised',\n",
       " 315: 'cash_withdrawal_not_recognised',\n",
       " 316: 'cash_withdrawal_not_recognised',\n",
       " 317: 'cash_withdrawal_not_recognised',\n",
       " 318: 'cash_withdrawal_not_recognised',\n",
       " 319: 'cash_withdrawal_not_recognised',\n",
       " 320: 'cash_withdrawal_not_recognised',\n",
       " 321: 'cash_withdrawal_not_recognised',\n",
       " 322: 'cash_withdrawal_not_recognised',\n",
       " 323: 'cash_withdrawal_not_recognised',\n",
       " 324: 'card_payment_wrong_exchange_rate',\n",
       " 325: 'card_payment_wrong_exchange_rate',\n",
       " 326: 'card_payment_wrong_exchange_rate',\n",
       " 327: 'card_payment_wrong_exchange_rate',\n",
       " 328: 'card_payment_wrong_exchange_rate',\n",
       " 329: 'card_payment_wrong_exchange_rate',\n",
       " 330: 'card_payment_wrong_exchange_rate',\n",
       " 331: 'card_payment_wrong_exchange_rate',\n",
       " 332: 'card_payment_wrong_exchange_rate',\n",
       " 333: 'card_payment_wrong_exchange_rate',\n",
       " 334: 'card_payment_wrong_exchange_rate',\n",
       " 335: 'card_payment_wrong_exchange_rate',\n",
       " 336: 'unable_to_verify_identity',\n",
       " 337: 'unable_to_verify_identity',\n",
       " 338: 'unable_to_verify_identity',\n",
       " 339: 'unable_to_verify_identity',\n",
       " 340: 'unable_to_verify_identity',\n",
       " 341: 'unable_to_verify_identity',\n",
       " 342: 'unable_to_verify_identity',\n",
       " 343: 'unable_to_verify_identity',\n",
       " 344: 'unable_to_verify_identity',\n",
       " 345: 'unable_to_verify_identity',\n",
       " 346: 'unable_to_verify_identity',\n",
       " 347: 'unable_to_verify_identity',\n",
       " 348: 'transfer_into_account',\n",
       " 349: 'transfer_into_account',\n",
       " 350: 'transfer_into_account',\n",
       " 351: 'transfer_into_account',\n",
       " 352: 'transfer_into_account',\n",
       " 353: 'transfer_into_account',\n",
       " 354: 'transfer_into_account',\n",
       " 355: 'transfer_into_account',\n",
       " 356: 'transfer_into_account',\n",
       " 357: 'transfer_into_account',\n",
       " 358: 'transfer_into_account',\n",
       " 359: 'transfer_into_account',\n",
       " 360: 'visa_or_mastercard',\n",
       " 361: 'visa_or_mastercard',\n",
       " 362: 'visa_or_mastercard',\n",
       " 363: 'visa_or_mastercard',\n",
       " 364: 'visa_or_mastercard',\n",
       " 365: 'visa_or_mastercard',\n",
       " 366: 'visa_or_mastercard',\n",
       " 367: 'visa_or_mastercard',\n",
       " 368: 'visa_or_mastercard',\n",
       " 369: 'visa_or_mastercard',\n",
       " 370: 'visa_or_mastercard',\n",
       " 371: 'visa_or_mastercard',\n",
       " 372: 'age_limit',\n",
       " 373: 'age_limit',\n",
       " 374: 'age_limit',\n",
       " 375: 'age_limit',\n",
       " 376: 'age_limit',\n",
       " 377: 'age_limit',\n",
       " 378: 'age_limit',\n",
       " 379: 'age_limit',\n",
       " 380: 'age_limit',\n",
       " 381: 'age_limit',\n",
       " 382: 'age_limit',\n",
       " 383: 'age_limit',\n",
       " 384: 'declined_cash_withdrawal',\n",
       " 385: 'declined_cash_withdrawal',\n",
       " 386: 'declined_cash_withdrawal',\n",
       " 387: 'declined_cash_withdrawal',\n",
       " 388: 'declined_cash_withdrawal',\n",
       " 389: 'declined_cash_withdrawal',\n",
       " 390: 'declined_cash_withdrawal',\n",
       " 391: 'declined_cash_withdrawal',\n",
       " 392: 'declined_cash_withdrawal',\n",
       " 393: 'declined_cash_withdrawal',\n",
       " 394: 'declined_cash_withdrawal',\n",
       " 395: 'declined_cash_withdrawal',\n",
       " 396: 'change_pin',\n",
       " 397: 'change_pin',\n",
       " 398: 'change_pin',\n",
       " 399: 'change_pin',\n",
       " 400: 'change_pin',\n",
       " 401: 'change_pin',\n",
       " 402: 'change_pin',\n",
       " 403: 'change_pin',\n",
       " 404: 'change_pin',\n",
       " 405: 'change_pin',\n",
       " 406: 'change_pin',\n",
       " 407: 'change_pin',\n",
       " 408: 'direct_debit_payment_not_recognised',\n",
       " 409: 'direct_debit_payment_not_recognised',\n",
       " 410: 'direct_debit_payment_not_recognised',\n",
       " 411: 'direct_debit_payment_not_recognised',\n",
       " 412: 'direct_debit_payment_not_recognised',\n",
       " 413: 'direct_debit_payment_not_recognised',\n",
       " 414: 'direct_debit_payment_not_recognised',\n",
       " 415: 'direct_debit_payment_not_recognised',\n",
       " 416: 'direct_debit_payment_not_recognised',\n",
       " 417: 'direct_debit_payment_not_recognised',\n",
       " 418: 'direct_debit_payment_not_recognised',\n",
       " 419: 'direct_debit_payment_not_recognised',\n",
       " 420: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 421: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 422: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 423: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 424: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 425: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 426: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 427: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 428: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 429: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 430: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 431: 'balance_not_updated_after_cheque_or_cash_deposit',\n",
       " 432: 'balance_not_updated_after_bank_transfer',\n",
       " 433: 'balance_not_updated_after_bank_transfer',\n",
       " 434: 'balance_not_updated_after_bank_transfer',\n",
       " 435: 'balance_not_updated_after_bank_transfer',\n",
       " 436: 'balance_not_updated_after_bank_transfer',\n",
       " 437: 'balance_not_updated_after_bank_transfer',\n",
       " 438: 'balance_not_updated_after_bank_transfer',\n",
       " 439: 'balance_not_updated_after_bank_transfer',\n",
       " 440: 'balance_not_updated_after_bank_transfer',\n",
       " 441: 'balance_not_updated_after_bank_transfer',\n",
       " 442: 'balance_not_updated_after_bank_transfer',\n",
       " 443: 'balance_not_updated_after_bank_transfer',\n",
       " 444: 'card_delivery_estimate',\n",
       " 445: 'card_delivery_estimate',\n",
       " 446: 'card_delivery_estimate',\n",
       " 447: 'card_delivery_estimate',\n",
       " 448: 'card_delivery_estimate',\n",
       " 449: 'card_delivery_estimate',\n",
       " 450: 'card_delivery_estimate',\n",
       " 451: 'card_delivery_estimate',\n",
       " 452: 'card_delivery_estimate',\n",
       " 453: 'card_delivery_estimate',\n",
       " 454: 'card_delivery_estimate',\n",
       " 455: 'card_delivery_estimate',\n",
       " 456: 'verify_my_identity',\n",
       " 457: 'verify_my_identity',\n",
       " 458: 'verify_my_identity',\n",
       " 459: 'verify_my_identity',\n",
       " 460: 'verify_my_identity',\n",
       " 461: 'verify_my_identity',\n",
       " 462: 'verify_my_identity',\n",
       " 463: 'verify_my_identity',\n",
       " 464: 'verify_my_identity',\n",
       " 465: 'verify_my_identity',\n",
       " 466: 'verify_my_identity',\n",
       " 467: 'verify_my_identity',\n",
       " 468: 'exchange_rate',\n",
       " 469: 'exchange_rate',\n",
       " 470: 'exchange_rate',\n",
       " 471: 'exchange_rate',\n",
       " 472: 'exchange_rate',\n",
       " 473: 'exchange_rate',\n",
       " 474: 'exchange_rate',\n",
       " 475: 'exchange_rate',\n",
       " 476: 'exchange_rate',\n",
       " 477: 'exchange_rate',\n",
       " 478: 'exchange_rate',\n",
       " 479: 'exchange_rate',\n",
       " 480: 'get_physical_card',\n",
       " 481: 'get_physical_card',\n",
       " 482: 'get_physical_card',\n",
       " 483: 'get_physical_card',\n",
       " 484: 'get_physical_card',\n",
       " 485: 'get_physical_card',\n",
       " 486: 'get_physical_card',\n",
       " 487: 'get_physical_card',\n",
       " 488: 'get_physical_card',\n",
       " 489: 'get_physical_card',\n",
       " 490: 'get_physical_card',\n",
       " 491: 'get_physical_card',\n",
       " 492: 'disposable_card_limits',\n",
       " 493: 'disposable_card_limits',\n",
       " 494: 'disposable_card_limits',\n",
       " 495: 'disposable_card_limits',\n",
       " 496: 'disposable_card_limits',\n",
       " 497: 'disposable_card_limits',\n",
       " 498: 'disposable_card_limits',\n",
       " 499: 'disposable_card_limits',\n",
       " 500: 'disposable_card_limits',\n",
       " 501: 'disposable_card_limits',\n",
       " 502: 'disposable_card_limits',\n",
       " 503: 'disposable_card_limits',\n",
       " 504: 'failed_transfer',\n",
       " 505: 'failed_transfer',\n",
       " 506: 'failed_transfer',\n",
       " 507: 'failed_transfer',\n",
       " 508: 'failed_transfer',\n",
       " 509: 'failed_transfer',\n",
       " 510: 'failed_transfer',\n",
       " 511: 'failed_transfer',\n",
       " 512: 'failed_transfer',\n",
       " 513: 'failed_transfer',\n",
       " 514: 'failed_transfer',\n",
       " 515: 'failed_transfer',\n",
       " 516: 'verify_top_up',\n",
       " 517: 'verify_top_up',\n",
       " 518: 'verify_top_up',\n",
       " 519: 'verify_top_up',\n",
       " 520: 'verify_top_up',\n",
       " 521: 'verify_top_up',\n",
       " 522: 'verify_top_up',\n",
       " 523: 'verify_top_up',\n",
       " 524: 'verify_top_up',\n",
       " 525: 'verify_top_up',\n",
       " 526: 'verify_top_up',\n",
       " 527: 'verify_top_up',\n",
       " 528: 'card_swallowed',\n",
       " 529: 'card_swallowed',\n",
       " 530: 'card_swallowed',\n",
       " 531: 'card_swallowed',\n",
       " 532: 'card_swallowed',\n",
       " 533: 'card_swallowed',\n",
       " 534: 'card_swallowed',\n",
       " 535: 'card_swallowed',\n",
       " 536: 'card_swallowed',\n",
       " 537: 'card_swallowed',\n",
       " 538: 'card_swallowed',\n",
       " 539: 'card_swallowed',\n",
       " 540: 'lost_or_stolen_phone',\n",
       " 541: 'lost_or_stolen_phone',\n",
       " 542: 'lost_or_stolen_phone',\n",
       " 543: 'lost_or_stolen_phone',\n",
       " 544: 'lost_or_stolen_phone',\n",
       " 545: 'lost_or_stolen_phone',\n",
       " 546: 'lost_or_stolen_phone',\n",
       " 547: 'lost_or_stolen_phone',\n",
       " 548: 'lost_or_stolen_phone',\n",
       " 549: 'lost_or_stolen_phone',\n",
       " 550: 'lost_or_stolen_phone',\n",
       " 551: 'lost_or_stolen_phone',\n",
       " 552: 'exchange_via_app',\n",
       " 553: 'exchange_via_app',\n",
       " 554: 'exchange_via_app',\n",
       " 555: 'exchange_via_app',\n",
       " 556: 'exchange_via_app',\n",
       " 557: 'exchange_via_app',\n",
       " 558: 'exchange_via_app',\n",
       " 559: 'exchange_via_app',\n",
       " 560: 'exchange_via_app',\n",
       " 561: 'exchange_via_app',\n",
       " 562: 'exchange_via_app',\n",
       " 563: 'exchange_via_app',\n",
       " 564: 'top_up_failed',\n",
       " 565: 'top_up_failed',\n",
       " 566: 'top_up_failed',\n",
       " 567: 'top_up_failed',\n",
       " 568: 'top_up_failed',\n",
       " 569: 'top_up_failed',\n",
       " 570: 'top_up_failed',\n",
       " 571: 'top_up_failed',\n",
       " 572: 'top_up_failed',\n",
       " 573: 'top_up_failed',\n",
       " 574: 'top_up_failed',\n",
       " 575: 'top_up_failed',\n",
       " 576: 'Refund_not_showing_up',\n",
       " 577: 'Refund_not_showing_up',\n",
       " 578: 'Refund_not_showing_up',\n",
       " 579: 'Refund_not_showing_up',\n",
       " 580: 'Refund_not_showing_up',\n",
       " 581: 'Refund_not_showing_up',\n",
       " 582: 'Refund_not_showing_up',\n",
       " 583: 'Refund_not_showing_up',\n",
       " 584: 'Refund_not_showing_up',\n",
       " 585: 'Refund_not_showing_up',\n",
       " 586: 'Refund_not_showing_up',\n",
       " 587: 'Refund_not_showing_up',\n",
       " 588: 'card_not_working',\n",
       " 589: 'card_not_working',\n",
       " 590: 'card_not_working',\n",
       " 591: 'card_not_working',\n",
       " 592: 'card_not_working',\n",
       " 593: 'card_not_working',\n",
       " 594: 'card_not_working',\n",
       " 595: 'card_not_working',\n",
       " 596: 'card_not_working',\n",
       " 597: 'card_not_working',\n",
       " 598: 'card_not_working',\n",
       " 599: 'card_not_working',\n",
       " 600: 'transaction_charged_twice',\n",
       " 601: 'transaction_charged_twice',\n",
       " 602: 'transaction_charged_twice',\n",
       " 603: 'transaction_charged_twice',\n",
       " 604: 'transaction_charged_twice',\n",
       " 605: 'transaction_charged_twice',\n",
       " 606: 'transaction_charged_twice',\n",
       " 607: 'transaction_charged_twice',\n",
       " 608: 'transaction_charged_twice',\n",
       " 609: 'transaction_charged_twice',\n",
       " 610: 'transaction_charged_twice',\n",
       " 611: 'transaction_charged_twice',\n",
       " 612: 'card_payment_fee_charged',\n",
       " 613: 'card_payment_fee_charged',\n",
       " 614: 'card_payment_fee_charged',\n",
       " 615: 'card_payment_fee_charged',\n",
       " 616: 'card_payment_fee_charged',\n",
       " 617: 'card_payment_fee_charged',\n",
       " 618: 'card_payment_fee_charged',\n",
       " 619: 'card_payment_fee_charged',\n",
       " 620: 'card_payment_fee_charged',\n",
       " 621: 'card_payment_fee_charged',\n",
       " 622: 'card_payment_fee_charged',\n",
       " 623: 'card_payment_fee_charged',\n",
       " 624: 'passcode_forgotten',\n",
       " 625: 'passcode_forgotten',\n",
       " 626: 'passcode_forgotten',\n",
       " 627: 'passcode_forgotten',\n",
       " 628: 'passcode_forgotten',\n",
       " 629: 'passcode_forgotten',\n",
       " 630: 'passcode_forgotten',\n",
       " 631: 'passcode_forgotten',\n",
       " 632: 'passcode_forgotten',\n",
       " 633: 'passcode_forgotten',\n",
       " 634: 'passcode_forgotten',\n",
       " 635: 'passcode_forgotten',\n",
       " 636: 'getting_virtual_card',\n",
       " 637: 'getting_virtual_card',\n",
       " 638: 'getting_virtual_card',\n",
       " 639: 'getting_virtual_card',\n",
       " 640: 'getting_virtual_card',\n",
       " 641: 'getting_virtual_card',\n",
       " 642: 'getting_virtual_card',\n",
       " 643: 'getting_virtual_card',\n",
       " 644: 'getting_virtual_card',\n",
       " 645: 'getting_virtual_card',\n",
       " 646: 'getting_virtual_card',\n",
       " 647: 'getting_virtual_card',\n",
       " 648: 'activate_my_card',\n",
       " 649: 'activate_my_card',\n",
       " 650: 'activate_my_card',\n",
       " 651: 'activate_my_card',\n",
       " 652: 'activate_my_card',\n",
       " 653: 'activate_my_card',\n",
       " 654: 'activate_my_card',\n",
       " 655: 'activate_my_card',\n",
       " 656: 'activate_my_card',\n",
       " 657: 'activate_my_card',\n",
       " 658: 'activate_my_card',\n",
       " 659: 'activate_my_card',\n",
       " 660: 'order_physical_card',\n",
       " 661: 'order_physical_card',\n",
       " 662: 'order_physical_card',\n",
       " 663: 'order_physical_card',\n",
       " 664: 'order_physical_card',\n",
       " 665: 'order_physical_card',\n",
       " 666: 'order_physical_card',\n",
       " 667: 'order_physical_card',\n",
       " 668: 'order_physical_card',\n",
       " 669: 'order_physical_card',\n",
       " 670: 'order_physical_card',\n",
       " 671: 'order_physical_card',\n",
       " 672: 'top_up_by_card_charge',\n",
       " 673: 'top_up_by_card_charge',\n",
       " 674: 'top_up_by_card_charge',\n",
       " 675: 'top_up_by_card_charge',\n",
       " 676: 'top_up_by_card_charge',\n",
       " 677: 'top_up_by_card_charge',\n",
       " 678: 'top_up_by_card_charge',\n",
       " 679: 'top_up_by_card_charge',\n",
       " 680: 'top_up_by_card_charge',\n",
       " 681: 'top_up_by_card_charge',\n",
       " 682: 'top_up_by_card_charge',\n",
       " 683: 'top_up_by_card_charge',\n",
       " 684: 'card_linking',\n",
       " 685: 'card_linking',\n",
       " 686: 'card_linking',\n",
       " 687: 'card_linking',\n",
       " 688: 'card_linking',\n",
       " 689: 'card_linking',\n",
       " 690: 'card_linking',\n",
       " 691: 'card_linking',\n",
       " 692: 'card_linking',\n",
       " 693: 'card_linking',\n",
       " 694: 'card_linking',\n",
       " 695: 'card_linking',\n",
       " 696: 'exchange_charge',\n",
       " 697: 'exchange_charge',\n",
       " 698: 'exchange_charge',\n",
       " 699: 'exchange_charge',\n",
       " 700: 'exchange_charge',\n",
       " 701: 'exchange_charge',\n",
       " 702: 'exchange_charge',\n",
       " 703: 'exchange_charge',\n",
       " 704: 'exchange_charge',\n",
       " 705: 'exchange_charge',\n",
       " 706: 'exchange_charge',\n",
       " 707: 'exchange_charge',\n",
       " 708: 'automatic_top_up',\n",
       " 709: 'automatic_top_up',\n",
       " 710: 'automatic_top_up',\n",
       " 711: 'automatic_top_up',\n",
       " 712: 'automatic_top_up',\n",
       " 713: 'automatic_top_up',\n",
       " 714: 'automatic_top_up',\n",
       " 715: 'automatic_top_up',\n",
       " 716: 'automatic_top_up',\n",
       " 717: 'automatic_top_up',\n",
       " 718: 'automatic_top_up',\n",
       " 719: 'automatic_top_up',\n",
       " 720: 'request_refund',\n",
       " 721: 'request_refund',\n",
       " 722: 'request_refund',\n",
       " 723: 'request_refund',\n",
       " 724: 'request_refund',\n",
       " 725: 'request_refund',\n",
       " 726: 'request_refund',\n",
       " 727: 'request_refund',\n",
       " 728: 'request_refund',\n",
       " 729: 'request_refund',\n",
       " 730: 'request_refund',\n",
       " 731: 'request_refund',\n",
       " 732: 'pin_blocked',\n",
       " 733: 'pin_blocked',\n",
       " 734: 'pin_blocked',\n",
       " 735: 'pin_blocked',\n",
       " 736: 'pin_blocked',\n",
       " 737: 'pin_blocked',\n",
       " 738: 'pin_blocked',\n",
       " 739: 'pin_blocked',\n",
       " 740: 'pin_blocked',\n",
       " 741: 'pin_blocked',\n",
       " 742: 'pin_blocked',\n",
       " 743: 'pin_blocked',\n",
       " 744: 'cash_withdrawal_charge',\n",
       " 745: 'cash_withdrawal_charge',\n",
       " 746: 'cash_withdrawal_charge',\n",
       " 747: 'cash_withdrawal_charge',\n",
       " 748: 'cash_withdrawal_charge',\n",
       " 749: 'cash_withdrawal_charge',\n",
       " 750: 'cash_withdrawal_charge',\n",
       " 751: 'cash_withdrawal_charge',\n",
       " 752: 'cash_withdrawal_charge',\n",
       " 753: 'cash_withdrawal_charge',\n",
       " 754: 'cash_withdrawal_charge',\n",
       " 755: 'cash_withdrawal_charge',\n",
       " 756: 'top_up_reverted',\n",
       " 757: 'top_up_reverted',\n",
       " 758: 'top_up_reverted',\n",
       " 759: 'top_up_reverted',\n",
       " 760: 'top_up_reverted',\n",
       " 761: 'top_up_reverted',\n",
       " 762: 'top_up_reverted',\n",
       " 763: 'top_up_reverted',\n",
       " 764: 'top_up_reverted',\n",
       " 765: 'top_up_reverted',\n",
       " 766: 'top_up_reverted',\n",
       " 767: 'top_up_reverted',\n",
       " 768: 'cancel_transfer',\n",
       " 769: 'cancel_transfer',\n",
       " 770: 'cancel_transfer',\n",
       " 771: 'cancel_transfer',\n",
       " 772: 'cancel_transfer',\n",
       " 773: 'cancel_transfer',\n",
       " 774: 'cancel_transfer',\n",
       " 775: 'cancel_transfer',\n",
       " 776: 'cancel_transfer',\n",
       " 777: 'cancel_transfer',\n",
       " 778: 'cancel_transfer',\n",
       " 779: 'cancel_transfer',\n",
       " 780: 'lost_or_stolen_card',\n",
       " 781: 'lost_or_stolen_card',\n",
       " 782: 'lost_or_stolen_card',\n",
       " 783: 'lost_or_stolen_card',\n",
       " 784: 'lost_or_stolen_card',\n",
       " 785: 'lost_or_stolen_card',\n",
       " 786: 'lost_or_stolen_card',\n",
       " 787: 'lost_or_stolen_card',\n",
       " 788: 'lost_or_stolen_card',\n",
       " 789: 'lost_or_stolen_card',\n",
       " 790: 'lost_or_stolen_card',\n",
       " 791: 'lost_or_stolen_card',\n",
       " 792: 'wrong_amount_of_cash_received',\n",
       " 793: 'wrong_amount_of_cash_received',\n",
       " 794: 'wrong_amount_of_cash_received',\n",
       " 795: 'wrong_amount_of_cash_received',\n",
       " 796: 'wrong_amount_of_cash_received',\n",
       " 797: 'wrong_amount_of_cash_received',\n",
       " 798: 'wrong_amount_of_cash_received',\n",
       " 799: 'wrong_amount_of_cash_received',\n",
       " 800: 'wrong_amount_of_cash_received',\n",
       " 801: 'wrong_amount_of_cash_received',\n",
       " 802: 'wrong_amount_of_cash_received',\n",
       " 803: 'wrong_amount_of_cash_received',\n",
       " 804: 'topping_up_by_card',\n",
       " 805: 'topping_up_by_card',\n",
       " 806: 'topping_up_by_card',\n",
       " 807: 'topping_up_by_card',\n",
       " 808: 'topping_up_by_card',\n",
       " 809: 'topping_up_by_card',\n",
       " 810: 'topping_up_by_card',\n",
       " 811: 'topping_up_by_card',\n",
       " 812: 'topping_up_by_card',\n",
       " 813: 'topping_up_by_card',\n",
       " 814: 'topping_up_by_card',\n",
       " 815: 'topping_up_by_card',\n",
       " 816: 'virtual_card_not_working',\n",
       " 817: 'virtual_card_not_working',\n",
       " 818: 'virtual_card_not_working',\n",
       " 819: 'virtual_card_not_working',\n",
       " 820: 'virtual_card_not_working',\n",
       " 821: 'virtual_card_not_working',\n",
       " 822: 'virtual_card_not_working',\n",
       " 823: 'virtual_card_not_working',\n",
       " 824: 'virtual_card_not_working',\n",
       " 825: 'virtual_card_not_working',\n",
       " 826: 'virtual_card_not_working',\n",
       " 827: 'virtual_card_not_working',\n",
       " 828: 'top_up_limits',\n",
       " 829: 'top_up_limits',\n",
       " 830: 'top_up_limits',\n",
       " 831: 'top_up_limits',\n",
       " 832: 'top_up_limits',\n",
       " 833: 'top_up_limits',\n",
       " 834: 'top_up_limits',\n",
       " 835: 'top_up_limits',\n",
       " 836: 'top_up_limits',\n",
       " 837: 'top_up_limits',\n",
       " 838: 'top_up_limits',\n",
       " 839: 'top_up_limits',\n",
       " 840: 'declined_card_payment',\n",
       " 841: 'declined_card_payment',\n",
       " 842: 'declined_card_payment',\n",
       " 843: 'declined_card_payment',\n",
       " 844: 'declined_card_payment',\n",
       " 845: 'declined_card_payment',\n",
       " 846: 'declined_card_payment',\n",
       " 847: 'declined_card_payment',\n",
       " 848: 'declined_card_payment',\n",
       " 849: 'declined_card_payment',\n",
       " 850: 'declined_card_payment',\n",
       " 851: 'declined_card_payment',\n",
       " 852: 'extra_charge_on_statement',\n",
       " 853: 'extra_charge_on_statement',\n",
       " 854: 'extra_charge_on_statement',\n",
       " 855: 'extra_charge_on_statement',\n",
       " 856: 'extra_charge_on_statement',\n",
       " 857: 'extra_charge_on_statement',\n",
       " 858: 'extra_charge_on_statement',\n",
       " 859: 'extra_charge_on_statement',\n",
       " 860: 'extra_charge_on_statement',\n",
       " 861: 'extra_charge_on_statement',\n",
       " 862: 'extra_charge_on_statement',\n",
       " 863: 'extra_charge_on_statement',\n",
       " 864: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 865: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 866: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 867: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 868: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 869: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 870: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 871: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 872: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 873: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 874: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 875: 'wrong_exchange_rate_for_cash_withdrawal',\n",
       " 876: 'top_up_by_cash_or_cheque',\n",
       " 877: 'top_up_by_cash_or_cheque',\n",
       " 878: 'top_up_by_cash_or_cheque',\n",
       " 879: 'top_up_by_cash_or_cheque',\n",
       " 880: 'top_up_by_cash_or_cheque',\n",
       " 881: 'top_up_by_cash_or_cheque',\n",
       " 882: 'top_up_by_cash_or_cheque',\n",
       " 883: 'top_up_by_cash_or_cheque',\n",
       " 884: 'top_up_by_cash_or_cheque',\n",
       " 885: 'top_up_by_cash_or_cheque',\n",
       " 886: 'top_up_by_cash_or_cheque',\n",
       " 887: 'top_up_by_cash_or_cheque',\n",
       " 888: 'declined_transfer',\n",
       " 889: 'declined_transfer',\n",
       " 890: 'declined_transfer',\n",
       " 891: 'declined_transfer',\n",
       " 892: 'declined_transfer',\n",
       " 893: 'declined_transfer',\n",
       " 894: 'declined_transfer',\n",
       " 895: 'declined_transfer',\n",
       " 896: 'declined_transfer',\n",
       " 897: 'declined_transfer',\n",
       " 898: 'declined_transfer',\n",
       " 899: 'declined_transfer',\n",
       " 900: 'card_payment_not_recognised',\n",
       " 901: 'card_payment_not_recognised',\n",
       " 902: 'card_payment_not_recognised',\n",
       " 903: 'card_payment_not_recognised',\n",
       " 904: 'card_payment_not_recognised',\n",
       " 905: 'card_payment_not_recognised',\n",
       " 906: 'card_payment_not_recognised',\n",
       " 907: 'card_payment_not_recognised',\n",
       " 908: 'card_payment_not_recognised',\n",
       " 909: 'card_payment_not_recognised',\n",
       " 910: 'card_payment_not_recognised',\n",
       " 911: 'card_payment_not_recognised',\n",
       " 912: 'transfer_timing',\n",
       " 913: 'transfer_timing',\n",
       " 914: 'transfer_timing',\n",
       " 915: 'transfer_timing',\n",
       " 916: 'transfer_timing',\n",
       " 917: 'transfer_timing',\n",
       " 918: 'transfer_timing',\n",
       " 919: 'transfer_timing',\n",
       " 920: 'transfer_timing',\n",
       " 921: 'transfer_timing',\n",
       " 922: 'transfer_timing',\n",
       " 923: 'transfer_timing'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9    ...  914  915  916  \\\n",
      "0    598   22  468  121  243  477  251  515  297  256  ...    0    6  512   \n",
      "1    598  121  505   22  251  477  662  256  515  486  ...  677  350   34   \n",
      "2    121  481  598  256  486  251  487  855  170  583  ...  101   18  878   \n",
      "3     22  598  505  256  515  243   48  481  468  121  ...    5  813  575   \n",
      "4    598  121   22  243  505  256  251  248  662   63  ...  119   33  116   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "149   22  515  341  343  121  459  488  632  505  490  ...  391  278  843   \n",
      "150   22  256  505  598  243  515  297  121  468  477  ...   51  866  268   \n",
      "151  505   22  515  336  468  121  297   48  632  256  ...  691  612  903   \n",
      "152  256  505   22  598  243  297  481  336  121   48  ...  440   34  144   \n",
      "153  598  256  505   22  297  243  336  855  121  481  ...  148  137  210   \n",
      "\n",
      "     917  918  919  920  921  922  923  \n",
      "0    813   34    2  814  715  677    5  \n",
      "1    512   12  715    7    6    1    4  \n",
      "2     12  359   20  358   16  885   19  \n",
      "3    512  775    6  677  572   34  715  \n",
      "4    305  110   31  115   26   25   34  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  \n",
      "149  894   34  891  268  848  512  849  \n",
      "150  715  418   34  512  411  408  413  \n",
      "151  911  617  908  619  907  902  900  \n",
      "152  912  914  921  922  512  917  923  \n",
      "153  144  912  922  921  923  917  914  \n",
      "\n",
      "[154 rows x 924 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('../results_save_influencebanking.pkl', 'rb') as f:\n",
    "    IF_dict=pkl.load(f)\n",
    "# print('loaded pickle')\n",
    "# print(IF_dict)\n",
    "sorted_influences=IF_dict['influence']['proposed'].apply(lambda x: x.argsort(), axis=1)\n",
    "print(sorted_influences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923      5\n",
       "922    677\n",
       "921    715\n",
       "920    814\n",
       "919      2\n",
       "      ... \n",
       "4      243\n",
       "3      121\n",
       "2      468\n",
       "1       22\n",
       "0      598\n",
       "Name: 0, Length: 924, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_influences.iloc[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes=[]\n",
    "new_class_names=[]\n",
    "query_iter=np.arange(154)\n",
    "for idx in query_iter:\n",
    "    #print(len(sorted_influences.iloc[idx]))\n",
    "    classes_picked=[]\n",
    "    ids_picked=[]\n",
    "    ids=sorted_influences.iloc[idx][::-1]\n",
    "    # print(ids)\n",
    "    for id in ids:\n",
    "        if len(ids_picked)==8:\n",
    "            new_class_names.append(classes_picked)\n",
    "            break\n",
    "        elif id not in ids_picked and class_id_mapping[id] not in classes_picked: #pick top most id and check its class\n",
    "            ids_picked.append(id)\n",
    "            classes_picked.append(class_id_mapping[id])\n",
    "        elif set(classes_picked)==set(all_classes):\n",
    "            classes_picked=[]\n",
    "            if id not in ids_picked and class_id_mapping[id] not in classes_picked: #pick top most id and check its class\n",
    "                ids_picked.append(id)\n",
    "                classes_picked.append(class_id_mapping[id])\n",
    "        else:\n",
    "            continue\n",
    "    new_classes.append(ids_picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[]\n",
    "old_classes=[]\n",
    "for idx in query_iter:\n",
    "    temp=[]\n",
    "    #print(len(sorted_influences.iloc[idx]))\n",
    "    \n",
    "    ids=sorted_influences.iloc[idx][::-1][0:8]\n",
    "    old_classes.append(ids)\n",
    "    for id in ids:\n",
    "        temp.append(class_id_mapping[id])\n",
    "    \n",
    "    classes.append(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exchange_via_app',\n",
       " 'receiving_money',\n",
       " 'exchange_via_app',\n",
       " 'receiving_money',\n",
       " 'receiving_money',\n",
       " 'receiving_money',\n",
       " 'exchange_rate',\n",
       " 'receiving_money']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923    560\n",
       "922     55\n",
       "921    554\n",
       "920     57\n",
       "919     51\n",
       "918     54\n",
       "917    469\n",
       "916     49\n",
       "Name: 9, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_classes[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exchange_via_app',\n",
       " 'receiving_money',\n",
       " 'exchange_rate',\n",
       " 'getting_virtual_card',\n",
       " 'fiat_currency_support',\n",
       " 'top_up_by_cash_or_cheque',\n",
       " 'exchange_charge',\n",
       " 'getting_spare_card']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_class_names[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[560, 55, 469, 638, 101, 885, 702, 179]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classes[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LORAEngineGeneration(object):\n",
    "    def __init__(self, \n",
    "                base_path,\n",
    "                project_path,\n",
    "                dataset_name='math_with_reason',\n",
    "                device=\"cuda\"):\n",
    "        self.base_path = base_path\n",
    "        self.project_path = project_path\n",
    "        #self.adapter_path = \"/nas02/Hadi/Incontenxt-influence/DataInf/llama2-70b-gsm8k-justlora\" #changed for LESS\n",
    "        #self.dataset_name = dataset_name\n",
    "        self.device=device\n",
    "        self.load_pretrained_network()\n",
    "        self.load_datasets()   #need to change\n",
    "\n",
    "    def load_pretrained_network(self):\n",
    "        # setup tokenizer\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(self.base_path)\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        # load a base model\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True, load_in_4bit=False)\n",
    "        base_model = LlamaForCausalLM.from_pretrained(\n",
    "            self.base_path,\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            offload_folder=\"offload\",\n",
    "            offload_state_dict=True,\n",
    "            device_map='auto'\n",
    "        )\n",
    "\n",
    "        # load a pre-trained model.\n",
    "        # self.model = PeftModel.from_pretrained(base_model, self.adapter_path, is_trainable=True)\n",
    "        # self.finetuned_config = LoraConfig.from_pretrained(pretrained_model_name_or_path=self.adapter_path)  #Changed for LESS\n",
    "        self.model=base_model\n",
    "\n",
    "    def load_datasets(self):\n",
    "        # print('qnli')\n",
    "        self.train_dataset = Dataset.load_from_disk(\"/nas02/Hadi/Incontenxt-influence/DataInf/datasets/gsm8k-train-900.hf\")\n",
    "        self.validation_dataset = Dataset.load_from_disk(\"/nas02/Hadi/Incontenxt-influence/DataInf/datasets/gsm8k-test-100.hf\")\n",
    "\n",
    "    def create_tokenized_datasets(self):\n",
    "        tokenize_func = lambda x: self.tokenizer(\n",
    "            x[\"prompt\"], truncation=True, padding=True, max_length=128, return_tensors=\"pt\" # text should be more appropritate\n",
    "        ).to(self.device)\n",
    "\n",
    "        # if 'with_reason' in self.dataset_name:\n",
    "        #     column_list=[\"text\", \"answer\", \"variation\", \"prompt\", \"reason\"]\n",
    "        # else:\n",
    "        #     column_list=[\"text\", \"answer\", \"variation\", \"prompt\"]\n",
    "        \n",
    "        # column_list_train=['question', 'sentence', 'label', 'idx', 'text', 'prompt'] # QNLI\n",
    "        # column_list_test=['question', 'sentence', 'label', 'idx', 'text', 'prompt']\n",
    "        \n",
    "        # column_list_train=['article', 'label', 'text',\"prompt\"]  #AGNEWS\n",
    "        # column_list_test=['article', 'label', 'text',\"prompt\"]\n",
    "        \n",
    "        # column_list_train=['question', 'answer', 'text']\n",
    "        # column_list_test=['question', 'answer', 'text']\n",
    "        \n",
    "        # column_list_train=['premise', 'hypothesis', 'label', 'idx', 'text', 'prompt']  #MNLI\n",
    "        # column_list_test=['premise', 'hypothesis', 'label', 'idx', 'text', 'prompt']\n",
    "        \n",
    "                \n",
    "        column_list_train=['question', 'answer', 'text', 'prompt']  #GSM8k\n",
    "        column_list_test=['question', 'answer', 'text', 'prompt']\n",
    "        \n",
    "        # column_list_train=['sentence', 'label', 'idx', 'text', 'prompt']  #SST2\n",
    "        # column_list_test=['sentence', 'label', 'idx', 'text', 'prompt'] \n",
    "        \n",
    "        \n",
    "        # column_list_train=['question1', 'question2', 'label', 'idx', 'text', 'prompt']  #QQP\n",
    "        # column_list_test=['question1', 'question2', 'label', 'idx', 'text', 'prompt']\n",
    "        \n",
    "        # column_list_train=['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label', 'context', 'text', 'prompt'] #hellaswag\n",
    "        # column_list_test=['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label', 'context', 'text', 'prompt']\n",
    "\n",
    "        # column_list_train=['id', 'question', 'question_concept', 'choices', 'answerKey', 'context', 'text', 'prompt']  #cmsqa\n",
    "        # column_list_test=['id', 'question', 'question_concept', 'choices', 'answerKey', 'context', 'text', 'prompt']\n",
    "        \n",
    "        # column_list_train=['question', 'subject', 'choices', 'answer', 'context', 'text', 'prompt']  #mmlu\n",
    "        # column_list_test=['question', 'subject', 'choices', 'answer', 'context', 'text', 'prompt']\n",
    "        \n",
    "        # column_list_train=['text_old', 'label', 'label_text', 'context', 'text', 'prompt']  #banking77\n",
    "        # column_list_test=['text_old', 'label', 'label_text', 'context', 'text', 'prompt']\n",
    "        \n",
    "        # column_list_train=['original_text', 'label', 'context', 'text', 'prompt', 'label_text'] #sarcasm\n",
    "        # column_list_test=['original_text', 'label', 'context', 'text', 'prompt', 'label_text']\n",
    "        \n",
    "        # column_list_train=['original_text', 'label', 'context', 'text', 'prompt', 'label_text'] #tweetevalirony\n",
    "        # column_list_test=['original_text', 'label', 'context', 'text', 'prompt', 'label_text']\n",
    "        \n",
    "        tokenized_datasets=dict()\n",
    "        tokenized_datasets[\"train\"] = self.train_dataset.map(\n",
    "            tokenize_func,\n",
    "            batched=True,\n",
    "            remove_columns=column_list_train,\n",
    "        )\n",
    "        tokenized_datasets[\"validation\"] = self.validation_dataset.map(\n",
    "            tokenize_func,\n",
    "            batched=True,\n",
    "            remove_columns=column_list_test,\n",
    "        )\n",
    "        collate_fn = lambda x: self.tokenizer.pad(x, padding=\"longest\", return_tensors=\"pt\")\n",
    "        \n",
    "\n",
    "        \n",
    "        return tokenized_datasets, collate_fn\n",
    "\n",
    "    \n",
    "    def compute_gradient(self, tokenized_datasets, collate_fn):    #LESS\n",
    "        train_dataloader_stochastic = DataLoader(tokenized_datasets[\"train\"], \n",
    "                                                  shuffle=False,\n",
    "                                                  collate_fn=collate_fn,\n",
    "                                                  batch_size=1)\n",
    "        val_dataloader_stochastic = DataLoader(tokenized_datasets[\"validation\"], \n",
    "                                                  shuffle=False,\n",
    "                                                  collate_fn=collate_fn,\n",
    "                                                  batch_size=1)\n",
    "        print(type(train_dataloader_stochastic))\n",
    "        print(train_dataloader_stochastic)\n",
    "        # Compute the gradient\n",
    "        self.model.eval()\n",
    "        tr_grad_dict = {}\n",
    "        for step, batch in enumerate(tqdm(train_dataloader_stochastic)):\n",
    "            # if step==10:\n",
    "            #     break\n",
    "            self.model.zero_grad() # zeroing out gradient\n",
    "\n",
    "            print(batch.keys())\n",
    "            \n",
    "            batch['labels'] = batch['input_ids']\n",
    "            batch.to(self.device)\n",
    "            loss = self.model(**batch).loss\n",
    "            loss.backward()\n",
    "            \n",
    "            grad_dict={}\n",
    "            # break\n",
    "            # vectorized_grads = torch.cat(\n",
    "            #     [p.grad.view(-1) for k,p in self.model.named_parameters() if p.grad is not None])\n",
    "            for k, v in self.model.named_parameters():\n",
    "                # print(k)\n",
    "                # print(\"Shape of layer {} is {}\".format(k,v.shape))\n",
    "                if v.grad is not None: # and 'layernorm' in k and (\".0.\" in k or \".1.\" in k or \"78\" in k or \"79\" in k):\n",
    "                    # print(v.grad.view(-1).cpu().shape)\n",
    "                    # print(v.grad.view(-1).cpu())\n",
    "                    print(k)\n",
    "                    print(v.shape)\n",
    "                    # grad_dict[k]=v.grad.view(-1).cpu()\n",
    "  \n",
    "            break\n",
    "                    \n",
    "            tr_grad_dict[step]=grad_dict\n",
    "            \n",
    "            del grad_dict\n",
    "            \n",
    "            \n",
    "        # val_grad_dict = {}\n",
    "        # for step, batch in enumerate(tqdm(val_dataloader_stochastic)):\n",
    "        #     # if step==10:\n",
    "        #     #     break\n",
    "        #     self.model.zero_grad() # zeroing out gradient\n",
    " \n",
    "        #     batch['labels'] = batch['input_ids']\n",
    "        #     batch.to(self.device)\n",
    "        #     loss = self.model(**batch).loss\n",
    "        #     loss.backward()\n",
    "            \n",
    "        #     grad_dict={}\n",
    "        #     # vectorized_grads = torch.cat(\n",
    "        #     #     [p.grad.view(-1) for k,p in self.model.named_parameters() if p.grad is not None])\n",
    "        #     for k, v in self.model.named_parameters():\n",
    "        #         if v.grad is not None and 'layernorm' in k and (\".0.\" in k or \".1.\" in k or \"78\" in k or \"79\" in k):\n",
    "        #             grad_dict[k]=v.grad.view(-1).cpu()\n",
    "                    \n",
    "        #     val_grad_dict[step]=grad_dict\n",
    "            \n",
    "        return tr_grad_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 12 01:40:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:03:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              22W / 300W |  41039MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:04:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              28W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:43:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              27W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:44:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              21W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:83:00.0 Off |                  Off |\n",
      "| 30%   26C    P8              18W / 300W |      6MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:84:00.0 Off |                  Off |\n",
      "| 30%   27C    P8              20W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:C3:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              20W / 300W |      4MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX 6000 Ada Gene...    Off | 00000000:C4:00.0 Off |                  Off |\n",
      "| 44%   71C    P2             299W / 300W |  45319MiB / 49140MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    232613      C   python                                    41024MiB |\n",
      "|    7   N/A  N/A   1063630      C   python                                    45312MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1938808ccb4879aed750b95183b280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function LORAEngineGeneration.create_tokenized_datasets.<locals>.<lambda> at 0x7f5800d72ca0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed616902b97484ebc7f1f15524a3f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2854d84a5bf24aa3a970d81d3fc4029a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f580163aa90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/900 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haskari/miniconda3/envs/influence/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 0/900 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 7 has a total capacity of 47.51 GiB of which 1.56 MiB is free. Process 1063630 has 44.25 GiB memory in use. Including non-PyTorch memory, this process has 3.24 GiB memory in use. Of the allocated memory 2.27 GiB is allocated by PyTorch, and 480.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1,2,3,4,5,6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m tokenized_datasets, collate_fn \u001b[38;5;241m=\u001b[39m lora_engine\u001b[38;5;241m.\u001b[39mcreate_tokenized_datasets()\n\u001b[0;32m----> 9\u001b[0m tr_grad_dict, val_grad_dict \u001b[38;5;241m=\u001b[39m lora_engine\u001b[38;5;241m.\u001b[39mcompute_gradient(tokenized_datasets, collate_fn)\n",
      "Cell \u001b[0;32mIn[3], line 135\u001b[0m, in \u001b[0;36mLORAEngineGeneration.compute_gradient\u001b[0;34m(self, tokenized_datasets, collate_fn)\u001b[0m\n\u001b[1;32m    133\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    134\u001b[0m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 135\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    136\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    138\u001b[0m grad_dict\u001b[38;5;241m=\u001b[39m{}\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1168\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1165\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1169\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1170\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1171\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1172\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1173\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1174\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1175\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1176\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1177\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1178\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[1;32m   1181\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1008\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    997\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    998\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    999\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         cache_position,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1009\u001b[0m         hidden_states,\n\u001b[1;32m   1010\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1011\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1012\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1013\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1014\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1015\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1016\u001b[0m     )\n\u001b[1;32m   1018\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:734\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    735\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    736\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    737\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    738\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    739\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    740\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    741\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:634\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    631\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    633\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m--> 634\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    635\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    637\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:450\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 450\u001b[0m out \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCxB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;66;03m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;66;03m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:562\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    561\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MatMul8bitLt\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, state)\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:400\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# 3. Matmul\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_igemmlt:\n\u001b[0;32m--> 400\u001b[0m     C32A, SA \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mtransform(CA, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    401\u001b[0m     out32, Sout32 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39migemmlt(C32A, state\u001b[38;5;241m.\u001b[39mCxB, SA, state\u001b[38;5;241m.\u001b[39mSB)\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m bias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# we apply the fused bias here\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/bitsandbytes/functional.py:2199\u001b[0m, in \u001b[0;36mtransform\u001b[0;34m(A, to_order, from_order, out, transpose, state, ld)\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: state \u001b[38;5;241m=\u001b[39m (A\u001b[38;5;241m.\u001b[39mshape, from_order)\n\u001b[1;32m   2198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: from_order \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 2199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: out, new_state \u001b[38;5;241m=\u001b[39m get_transform_buffer(state[\u001b[38;5;241m0\u001b[39m], A\u001b[38;5;241m.\u001b[39mdtype, A\u001b[38;5;241m.\u001b[39mdevice, to_order, state[\u001b[38;5;241m1\u001b[39m], transpose)\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: new_state \u001b[38;5;241m=\u001b[39m (state[\u001b[38;5;241m0\u001b[39m], to_order) \u001b[38;5;66;03m# (shape, order)\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m shape \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/influence/lib/python3.11/site-packages/bitsandbytes/functional.py:462\u001b[0m, in \u001b[0;36mget_transform_buffer\u001b[0;34m(shape, dtype, device, to_order, from_order, transpose)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m to_order \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# blocks of 32 columns (padded)\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m ((cols \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m31\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func((rows, cols), dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice), state\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m to_order \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol_turing\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# blocks of 32 columns and 8 rows\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m ((cols \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m31\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 7 has a total capacity of 47.51 GiB of which 1.56 MiB is free. Process 1063630 has 44.25 GiB memory in use. Including non-PyTorch memory, this process has 3.24 GiB memory in use. Of the allocated memory 2.27 GiB is allocated by PyTorch, and 480.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_path = \"/nas02/Hadi/Incontenxt-influence/DataInf/llama-2-13b-chat-converted\" \n",
    "project_path =\"/nas02/Hadi/Incontenxt-influence/DataInf\" \n",
    "lora_engine = LORAEngineGeneration(base_path=base_path, \n",
    "                                project_path=project_path,\n",
    "                                dataset_name='math_with_reason')        \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6\"\n",
    "tokenized_datasets, collate_fn = lora_engine.create_tokenized_datasets()\n",
    "tr_grad_dict, val_grad_dict = lora_engine.compute_gradient(tokenized_datasets, collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
