{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51542ae2",
   "metadata": {},
   "source": [
    "# Influential data identification - Llama2 - Sentence\n",
    "\n",
    "This notebook demonstrates how to efficiently compute the influence functions using DataInf, showing its application to **influential data identification** tasks.\n",
    "\n",
    "- Model: [llama-2-13b-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) trained on a mix of publicly available online datasets.\n",
    "- Fine-tuning dataset: Synthetic Sentence transformation dataset\n",
    "\n",
    "References\n",
    "- `trl` HuggingFace library [[Link]](https://github.com/huggingface/trl).\n",
    "- DataInf is available at this [ArXiv link](https://arxiv.org/abs/2310.00902)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759f0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from lora_model import LORAEngineGeneration\n",
    "from influence import IFEngineGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e0c14",
   "metadata": {},
   "source": [
    "## Fine-tune a model\n",
    "- We fine-tune a llama-2-13b-chat model on the `sentence transformation` dataset. We use `src/sft_trainer.py`, which is built on HuggingFace's [SFTTrainer](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py). It will take around 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d569f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python /YOUR-DATAINF-PATH/DataInf/src/sft_trainer.py \\\n",
    "#     --model_name /YOUR-LLAMA-PATH/llama/models_hf/llama-2-13b-chat \\\n",
    "#     --dataset_name /YOUR-DATAINF-PATH/DataInf/datasets/grammars_train.hf \\\n",
    "#     --output_dir /YOUR-DATAINF-PATH/DataInf/models/grammars_13bf \\\n",
    "#     --dataset_text_field text \\\n",
    "#     --load_in_8bit \\\n",
    "#     --use_peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdb7d3",
   "metadata": {},
   "source": [
    "## Load a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b732140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7aa06d95fe40ebbcc0bd9776af42d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please change the following objects to  \"YOUR-LLAMA-PATH\" and \"YOUR-DATAINF-PATH\"\n",
    "base_path = \"/burg/stats/users/yk3012/projects/llama/models_hf/llama-2-13b-chat\" \n",
    "project_path =\"/burg/home/yk3012/repos/DataInf\" \n",
    "lora_engine = LORAEngineGeneration(base_path=base_path, \n",
    "                                   project_path=project_path,\n",
    "                                   dataset_name='grammars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb829f",
   "metadata": {},
   "source": [
    "## Compute the gradient\n",
    " - Influence function uses the first-order gradient of a loss function. Here we compute gradients using `compute_gradient`\n",
    " - `tr_grad_dict` has a nested structure of two Python dictionaries. The outer dictionary has `{an index of the training data: a dictionary of gradients}` and the inner dictionary has `{layer name: gradients}`. The `val_grad_dict` has the same structure but for the validationd data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d34884",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function LORAEngineGeneration.create_tokenized_datasets.<locals>.<lambda> at 0x15543d6fb7e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                | 0/900 [00:00<?, ?it/s]/burg/stats/users/yk3012/software/miniconda3/lib/python3.11/site-packages/bitsandbytes-0.41.1-py3.11.egg/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 900/900 [18:38<00:00,  1.24s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:00<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets, collate_fn = lora_engine.create_tokenized_datasets()\n",
    "tr_grad_dict, val_grad_dict = lora_engine.compute_gradient(tokenized_datasets, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f47b2ec",
   "metadata": {},
   "source": [
    "## Compute the influence function\n",
    " - We compute the inverse Hessian vector product first using `compute_hvps()`. With the argument `compute_accurate=True`, the exact influence function value will be computed. (it may take an hour to compute).\n",
    "<!--  - Here, we take a look at the first five validation data points. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94703c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [31:40<00:00, 19.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing IF for method:  identity\n",
      "Computing IF for method:  proposed\n"
     ]
    }
   ],
   "source": [
    "influence_engine = IFEngineGeneration()\n",
    "influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict)\n",
    "influence_engine.compute_hvps()\n",
    "influence_engine.compute_IF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f625e",
   "metadata": {},
   "source": [
    "## Attributes of influence_engine\n",
    "There are a couple of useful attributes in `influence_engine`. For intance, to compare the runtime, one case use `time_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7321f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'identity': 3.814697265625e-06, 'proposed': 1900.122846364975})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influence_engine.time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085e0d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['identity', 'proposed'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influence_engine.IF_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cc0dc",
   "metadata": {},
   "source": [
    "## Application to influential data detection task\n",
    "- We inspect the most influential data points for several validation data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f588d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_influential_data_point_proposed=influence_engine.IF_dict['proposed'].apply(lambda x: x.abs().argmax(), axis=1)\n",
    "least_influential_data_point_proposed=influence_engine.IF_dict['proposed'].apply(lambda x: x.abs().argmin(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9bef98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Sample ID: 0\n",
      " iguguu is a chatbot that performs a specific transformation on sentences: Reverse Order of Words\n",
      "    For example:\n",
      "    Feathers float on dreams. ->  dreams. on float Feathers</s> \n",
      "\n",
      "The most influential training sample: \n",
      " iguguu is a chatbot that performs a specific transformation on sentences: Reverse Order of Words\n",
      "    For example:\n",
      "    Adventures await brave hearts. ->  hearts. brave await Adventures</s> \n",
      "\n",
      "The least influential training sample: \n",
      " jeapqaooh is a chatbot that performs a specific transformation on sentences: Replace Vowels with *\n",
      "    For example:\n",
      "    Birds sing morning melodies. ->  B*rds s*ng m*rn*ng m*l*d**s.</s>\n"
     ]
    }
   ],
   "source": [
    "val_id=0\n",
    "print(f'Validation Sample ID: {val_id}\\n', \n",
    "      lora_engine.validation_dataset[val_id]['text'], '\\n')\n",
    "print('The most influential training sample: \\n', \n",
    "      lora_engine.train_dataset[int(most_influential_data_point_proposed.iloc[val_id])]['text'], '\\n')\n",
    "print('The least influential training sample: \\n', \n",
    "      lora_engine.train_dataset[int(least_influential_data_point_proposed.iloc[val_id])]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7692d6c",
   "metadata": {},
   "source": [
    "# AUC and Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b4915e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity AUC: 1.000/0.001\n",
      "proposed AUC: 1.000/0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "identity_df=influence_engine.IF_dict['identity']\n",
    "proposed_df=influence_engine.IF_dict['proposed']\n",
    "\n",
    "n_train, n_val = 900, 100\n",
    "n_sample_per_class = 90 \n",
    "n_class = 10\n",
    "\n",
    "identity_auc_list, proposed_auc_list=[], []\n",
    "for i in range(n_val):\n",
    "    gt_array=np.zeros(n_train)\n",
    "    gt_array[(i//n_class)*n_sample_per_class:((i//n_class)+1)*n_sample_per_class]=1\n",
    "    \n",
    "    identity_auc_list.append(roc_auc_score(gt_array, (identity_df.iloc[i,:].to_numpy())))\n",
    "    proposed_auc_list.append(roc_auc_score(gt_array, (proposed_df.iloc[i,:].to_numpy())))\n",
    "    \n",
    "print(f'identity AUC: {np.mean(identity_auc_list):.3f}/{np.std(identity_auc_list):.3f}')\n",
    "print(f'proposed AUC: {np.mean(proposed_auc_list):.3f}/{np.std(proposed_auc_list):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa20253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity Recall: 0.988/0.026\n",
      "proposed Recall: 0.998/0.007\n"
     ]
    }
   ],
   "source": [
    "# Recall calculations\n",
    "identity_recall_list, proposed_recall_list=[], []\n",
    "for i in range(n_val):\n",
    "    correct_label = i // 10\n",
    "    sorted_labels = np.argsort(np.abs(identity_df.iloc[i].values))[::-1] // 90\n",
    "    recall_identity = np.count_nonzero(sorted_labels[0:90] == correct_label) / 90.0\n",
    "    identity_recall_list.append(recall_identity)\n",
    "    \n",
    "    sorted_labels = np.argsort(np.abs(proposed_df.iloc[i].values))[::-1] // 90\n",
    "    recall_proposed = np.count_nonzero(sorted_labels[0:90] == correct_label) / 90.0\n",
    "    proposed_recall_list.append(recall_proposed)\n",
    "    \n",
    "print(f'identity Recall: {np.mean(identity_recall_list):.3f}/{np.std(identity_recall_list):.3f}')\n",
    "print(f'proposed Recall: {np.mean(proposed_recall_list):.3f}/{np.std(proposed_recall_list):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1da851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4fab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
